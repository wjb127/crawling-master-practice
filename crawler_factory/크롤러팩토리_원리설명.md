# ğŸ­ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ ì‘ë™ ì›ë¦¬ ì„¤ëª…

## ğŸ“Œ ê°œìš”
í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ëŠ” **í…œí”Œë¦¿ ê¸°ë°˜ ì½”ë“œ ìƒì„±(Template-based Code Generation)** ì‹œìŠ¤í…œìœ¼ë¡œ, ê³ ê° ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥ë°›ì•„ ìë™ìœ¼ë¡œ ì™„ì „í•œ í¬ë¡¤ëŸ¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìƒì„±í•©ë‹ˆë‹¤.

---

## ğŸ¯ í•µì‹¬ ê°œë…

### 1. Factory Pattern (íŒ©í† ë¦¬ íŒ¨í„´)
```python
# ê¸°ë³¸ êµ¬ì¡°
class CrawlerFactory:
    def create_custom_crawler(self, requirements):
        # 1. ìš”êµ¬ì‚¬í•­ ë¶„ì„
        # 2. í…œí”Œë¦¿ ì„ íƒ
        # 3. ì½”ë“œ ìƒì„±
        # 4. íŒ¨í‚¤ì§•
        return complete_package
```

### 2. í…œí”Œë¦¿ ì—”ì§„
- ë¯¸ë¦¬ ì •ì˜ëœ ì½”ë“œ í…œí”Œë¦¿
- ë³€ìˆ˜ ì¹˜í™˜ ë°©ì‹
- ì¡°ê±´ë¶€ ì½”ë“œ ìƒì„±

---

## ğŸ”„ ì „ì²´ ì‘ë™ í”„ë¡œì„¸ìŠ¤

```mermaid
graph TD
    A[ê³ ê° ìš”êµ¬ì‚¬í•­ ì…ë ¥] --> B[ìš”êµ¬ì‚¬í•­ ë¶„ì„]
    B --> C[í…œí”Œë¦¿ ì„ íƒ]
    C --> D[ì½”ë“œ ìƒì„±]
    D --> E[íŒŒì¼ ì‹œìŠ¤í…œ ìƒì„±]
    E --> F[EXE ë¹Œë“œ]
    F --> G[ì¸ìŠ¤í†¨ëŸ¬ ìƒì„±]
    G --> H[ZIP íŒ¨í‚¤ì§•]
    H --> I[ë‹¤ìš´ë¡œë“œ ì œê³µ]
```

---

## ğŸ“ ë‹¨ê³„ë³„ ìƒì„¸ ì„¤ëª…

### Step 1: ìš”êµ¬ì‚¬í•­ ì…ë ¥ ë° ë¶„ì„
```python
customer_request = {
    "project_name": "NaverCrawler",
    "target_sites": ["https://news.naver.com"],
    "data_fields": ["ì œëª©", "ë‚´ìš©", "ë‚ ì§œ"],
    "output_format": "excel",
    "crawling_method": "static"
}
```

**ë¶„ì„ ê³¼ì •:**
- ì‚¬ì´íŠ¸ ìœ í˜• íŒë‹¨ (ì •ì /ë™ì )
- í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²°ì •
- ìµœì  í¬ë¡¤ë§ ë°©ì‹ ì„ íƒ

### Step 2: í…œí”Œë¦¿ ê¸°ë°˜ ì½”ë“œ ìƒì„±

#### 2.1 í¬ë¡¤ëŸ¬ ì—”ì§„ í…œí”Œë¦¿
```python
# crawler_engine_template.py
CRAWLER_ENGINE_TEMPLATE = '''
import requests
from bs4 import BeautifulSoup
{additional_imports}

class {project_name}Engine:
    def __init__(self):
        self.target_sites = {target_sites}
        self.data_fields = {data_fields}
        
    def crawl(self, url):
        {crawling_logic}
        
    def save_data(self, data):
        {save_logic}
'''
```

#### 2.2 ë³€ìˆ˜ ì¹˜í™˜
```python
def generate_crawler_engine(requirements):
    template = CRAWLER_ENGINE_TEMPLATE
    
    # ë³€ìˆ˜ ì¹˜í™˜
    code = template.format(
        project_name=requirements['project_name'],
        target_sites=requirements['target_sites'],
        data_fields=requirements['data_fields'],
        additional_imports=get_imports(requirements),
        crawling_logic=generate_crawling_logic(requirements),
        save_logic=generate_save_logic(requirements)
    )
    
    return code
```

### Step 3: GUI ì¸í„°í˜ì´ìŠ¤ ìƒì„±

#### 3.1 ë™ì  UI êµ¬ì„±
```python
def generate_gui(requirements):
    # í•„ë“œ ìˆ˜ì— ë”°ë¼ UI ë ˆì´ì•„ì›ƒ ì¡°ì •
    num_fields = len(requirements['data_fields'])
    
    if num_fields <= 5:
        layout = "simple_layout"
    elif num_fields <= 10:
        layout = "tabbed_layout"
    else:
        layout = "scrollable_layout"
    
    return generate_tkinter_code(layout, requirements)
```

#### 3.2 ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ìƒì„±
```python
def generate_event_handlers(requirements):
    handlers = []
    
    # í¬ë¡¤ë§ ì‹œì‘ ë²„íŠ¼
    handlers.append('''
    def start_crawling(self):
        self.engine = {project_name}Engine()
        self.engine.crawl(self.url_entry.get())
    '''.format(project_name=requirements['project_name']))
    
    # ì €ì¥ ë²„íŠ¼ (í˜•ì‹ì— ë”°ë¼)
    if requirements['output_format'] == 'excel':
        handlers.append(generate_excel_save_handler())
    elif requirements['output_format'] == 'csv':
        handlers.append(generate_csv_save_handler())
    
    return '\n'.join(handlers)
```

### Step 4: ì¡°ê±´ë¶€ ê¸°ëŠ¥ ì¶”ê°€

#### 4.1 ë¡œê·¸ì¸ ê¸°ëŠ¥
```python
if requirements.get('login_required'):
    # ë¡œê·¸ì¸ ê´€ë ¨ ì½”ë“œ ì¶”ê°€
    login_code = '''
    def login(self, username, password):
        session = requests.Session()
        login_data = {
            'username': username,
            'password': password
        }
        session.post(login_url, data=login_data)
        return session
    '''
    inject_code(login_code)
```

#### 4.2 ìŠ¤ì¼€ì¤„ë§ ê¸°ëŠ¥
```python
if requirements.get('schedule_required'):
    # ìŠ¤ì¼€ì¤„ëŸ¬ ì½”ë“œ ì¶”ê°€
    scheduler_code = '''
    import schedule
    import time
    
    def scheduled_crawl():
        crawler.run()
    
    schedule.every(1).hours.do(scheduled_crawl)
    '''
    inject_code(scheduler_code)
```

### Step 5: ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

#### 5.1 PyInstaller ì„¤ì •
```python
def generate_build_script(requirements):
    spec_content = f'''
# {requirements['project_name']}.spec
a = Analysis(
    ['{requirements['project_name']}_gui.py'],
    pathex=[],
    binaries=[],
    datas=[('docs', 'docs')],
    hiddenimports={get_hidden_imports(requirements)},
)
exe = EXE(
    a.scripts,
    name='{requirements['project_name']}',
    icon='icon.ico',
    console=False
)
'''
    return spec_content
```

#### 5.2 ì˜ì¡´ì„± ë¶„ì„
```python
def get_hidden_imports(requirements):
    imports = ['requests', 'bs4']
    
    if requirements['crawling_method'] == 'dynamic':
        imports.extend(['selenium', 'webdriver_manager'])
    
    if requirements['output_format'] == 'excel':
        imports.extend(['pandas', 'openpyxl'])
    
    return imports
```

### Step 6: ì¸ìŠ¤í†¨ëŸ¬ ìƒì„±

#### 6.1 Inno Setup ìŠ¤í¬ë¦½íŠ¸
```python
def generate_installer_script(requirements):
    iss_template = '''
[Setup]
AppName={app_name}
AppVersion=1.0
DefaultDirName={{pf}}\\{app_name}
OutputBaseFilename={app_name}_Setup

[Files]
Source: "dist\\{exe_name}.exe"; DestDir: "{{app}}"
Source: "docs\\*"; DestDir: "{{app}}\\docs"

[Icons]
Name: "{{commondesktop}}\\{app_name}"; Filename: "{{app}}\\{exe_name}.exe"
'''
    
    return iss_template.format(
        app_name=requirements['project_name'],
        exe_name=requirements['project_name']
    )
```

---

## ğŸ§© í•µì‹¬ ê¸°ìˆ  ìš”ì†Œ

### 1. í…œí”Œë¦¿ ì—”ì§„ ì‹œìŠ¤í…œ
```python
class TemplateEngine:
    def __init__(self):
        self.templates = {
            'crawler_engine': CRAWLER_ENGINE_TEMPLATE,
            'gui_interface': GUI_TEMPLATE,
            'build_script': BUILD_TEMPLATE
        }
    
    def render(self, template_name, context):
        template = self.templates[template_name]
        return template.format(**context)
```

### 2. ì½”ë“œ ìƒì„± ì „ëµ

#### 2.1 ì •ì  ì‚¬ì´íŠ¸ìš©
```python
def generate_static_crawler(url, fields):
    return f'''
    response = requests.get("{url}")
    soup = BeautifulSoup(response.text, 'html.parser')
    
    data = {{}}
    {generate_field_extractors(fields)}
    return data
    '''
```

#### 2.2 ë™ì  ì‚¬ì´íŠ¸ìš©
```python
def generate_dynamic_crawler(url, fields):
    return f'''
    driver = webdriver.Chrome()
    driver.get("{url}")
    time.sleep(2)
    
    data = {{}}
    {generate_selenium_extractors(fields)}
    driver.quit()
    return data
    '''
```

### 3. íŒ¨í„´ ë§¤ì¹­ ì‹œìŠ¤í…œ

```python
class PatternMatcher:
    patterns = {
        'ë‰´ìŠ¤': {
            'selectors': {
                'ì œëª©': 'h1, h2, .title',
                'ë‚´ìš©': '.content, .article-body',
                'ë‚ ì§œ': '.date, time'
            }
        },
        'ì‡¼í•‘ëª°': {
            'selectors': {
                'ìƒí’ˆëª…': '.product-name',
                'ê°€ê²©': '.price',
                'ë¦¬ë·°': '.review-count'
            }
        }
    }
    
    def match_site_type(self, url):
        # URL íŒ¨í„´ìœ¼ë¡œ ì‚¬ì´íŠ¸ ìœ í˜• íŒë‹¨
        if 'news' in url or 'article' in url:
            return 'ë‰´ìŠ¤'
        elif 'shop' in url or 'product' in url:
            return 'ì‡¼í•‘ëª°'
        return 'general'
```

---

## ğŸ”§ ìµœì í™” ê¸°ë²•

### 1. ìºì‹± ì‹œìŠ¤í…œ
```python
template_cache = {}

def get_cached_template(template_id):
    if template_id not in template_cache:
        template_cache[template_id] = load_template(template_id)
    return template_cache[template_id]
```

### 2. ë³‘ë ¬ ì²˜ë¦¬
```python
async def generate_files_parallel(requirements):
    tasks = [
        generate_crawler_engine_async(requirements),
        generate_gui_async(requirements),
        generate_docs_async(requirements)
    ]
    
    results = await asyncio.gather(*tasks)
    return results
```

### 3. ì¦ë¶„ ë¹Œë“œ
```python
def incremental_build(project_path):
    # ë³€ê²½ëœ íŒŒì¼ë§Œ ë‹¤ì‹œ ë¹Œë“œ
    changed_files = detect_changes(project_path)
    
    if not changed_files:
        return use_cached_build()
    
    return rebuild_changed_components(changed_files)
```

---

## ğŸ’¡ ê³ ê¸‰ ê¸°ëŠ¥

### 1. AI ê¸°ë°˜ ì„ íƒì ì¶”ì²œ
```python
def suggest_selectors(url, field_name):
    # í˜ì´ì§€ ë¶„ì„
    soup = analyze_page(url)
    
    # í•„ë“œëª… ê¸°ë°˜ ì¶”ì²œ
    if 'ì œëª©' in field_name:
        candidates = soup.find_all(['h1', 'h2', 'h3'])
        return rank_by_relevance(candidates)
```

### 2. ìë™ ì—ëŸ¬ ì²˜ë¦¬
```python
def inject_error_handling(code):
    wrapped_code = f'''
    try:
        {code}
    except requests.RequestException as e:
        logging.error(f"Network error: {{e}}")
        retry_with_backoff()
    except Exception as e:
        logging.error(f"Unexpected error: {{e}}")
        save_partial_results()
    '''
    return wrapped_code
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
def add_performance_monitoring(crawler_code):
    return f'''
    import time
    
    start_time = time.time()
    {crawler_code}
    elapsed = time.time() - start_time
    
    log_performance_metrics({{
        'duration': elapsed,
        'items_crawled': len(results),
        'rate': len(results) / elapsed
    }})
    '''
```

---

## ğŸ“Š ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Web Interface (FastAPI)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Factory System Core              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Template   â”‚   Code Generator      â”‚ â”‚
â”‚  â”‚   Engine    â”‚                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼           â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Build   â”‚ â”‚ Package  â”‚ â”‚  Deploy  â”‚
â”‚  System  â”‚ â”‚  System  â”‚ â”‚  System  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ í•µì‹¬ ì¥ì 

1. **ì™„ì „ ìë™í™”**: ìš”êµ¬ì‚¬í•­ â†’ ì¸ìŠ¤í†¨ëŸ¬ê¹Œì§€ ìë™
2. **ë§ì¶¤í˜• ìƒì„±**: ê³ ê°ë³„ íŠ¹í™” ê¸°ëŠ¥
3. **í™•ì¥ ê°€ëŠ¥**: ìƒˆë¡œìš´ í…œí”Œë¦¿ ì¶”ê°€ ìš©ì´
4. **ìœ ì§€ë³´ìˆ˜ ê°„í¸**: í…œí”Œë¦¿ë§Œ ìˆ˜ì •í•˜ë©´ ì „ì²´ ì ìš©
5. **í’ˆì§ˆ ë³´ì¦**: ê²€ì¦ëœ í…œí”Œë¦¿ ì‚¬ìš©

---

## ğŸ”¬ ì‹¤ì œ ë™ì‘ ì˜ˆì‹œ

### ì…ë ¥
```json
{
  "project_name": "NaverNewsCrawler",
  "target_sites": ["https://news.naver.com"],
  "data_fields": ["ì œëª©", "ìš”ì•½", "ë‚ ì§œ"]
}
```

### ì¶œë ¥
```
NaverNewsCrawler/
â”œâ”€â”€ crawler_engine.py    (2,341 lines)
â”œâ”€â”€ crawler_gui.py        (1,876 lines)
â”œâ”€â”€ requirements.txt      (12 packages)
â”œâ”€â”€ build.bat            (ìë™ ë¹Œë“œ)
â”œâ”€â”€ installer.iss        (Inno Setup)
â””â”€â”€ docs/
    â””â”€â”€ ì‚¬ìš©ë²•.md        (ìë™ ìƒì„±ë¨)
```

---

**ì´ê²ƒì´ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ì˜ í•µì‹¬ ì›ë¦¬ì…ë‹ˆë‹¤!** ğŸš€