#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ­ í¬ë¡¤ë§ íˆ´ ê³µì¥ (Crawler Factory System)
ê³ ê° ìš”ì²­ì‚¬í•­ì„ ì…ë ¥í•˜ë©´ ë§ì¶¤í˜• í¬ë¡¤ë§ íˆ´ì„ ìë™ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import json
import shutil
from datetime import datetime
from typing import Dict, List, Any
import re

class CrawlerFactory:
    """í¬ë¡¤ë§ íˆ´ ìë™ ìƒì„± ê³µì¥"""
    
    def __init__(self):
        self.base_path = os.path.dirname(os.path.abspath(__file__))
        self.template_path = os.path.join(self.base_path, "templates")
        self.output_path = os.path.join(self.base_path, "generated")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.output_path, exist_ok=True)
        
    def create_custom_crawler(self, customer_request: Dict[str, Any]) -> str:
        """ê³ ê° ìš”ì²­ì‚¬í•­ì— ë”°ë¥¸ ë§ì¶¤í˜• í¬ë¡¤ëŸ¬ ìƒì„±"""
        
        # í”„ë¡œì íŠ¸ ì„¤ì •
        project_name = customer_request.get('project_name', 'CustomCrawler')
        safe_name = re.sub(r'[^a-zA-Z0-9]', '', project_name)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        project_dir = os.path.join(self.output_path, f"{safe_name}_{timestamp}")
        
        # í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(project_dir, exist_ok=True)
        
        # 1. ë©”ì¸ í¬ë¡¤ëŸ¬ ìƒì„±
        self._generate_main_crawler(project_dir, customer_request)
        
        # 2. GUI ì¸í„°í˜ì´ìŠ¤ ìƒì„±
        self._generate_gui(project_dir, customer_request)
        
        # 3. í”„ë¦¬ì…‹ ì„¤ì • ìƒì„±
        self._generate_presets(project_dir, customer_request)
        
        # 4. ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        self._generate_build_scripts(project_dir, customer_request)
        
        # 5. ì¸ìŠ¤í†¨ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        self._generate_installer(project_dir, customer_request)
        
        # 6. ë¬¸ì„œ ìƒì„±
        self._generate_documentation(project_dir, customer_request)
        
        # 7. ì•„ì´ì½˜ ë° ë¦¬ì†ŒìŠ¤ ìƒì„±
        self._generate_resources(project_dir, customer_request)
        
        print(f"âœ… í¬ë¡¤ëŸ¬ ìƒì„± ì™„ë£Œ: {project_dir}")
        return project_dir
    
    def _generate_main_crawler(self, project_dir: str, request: Dict):
        """ë©”ì¸ í¬ë¡¤ëŸ¬ ì—”ì§„ ìƒì„±"""
        
        crawler_code = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
{request.get('company_name', 'ê³ ê°ì‚¬')} ë§ì¶¤í˜• í¬ë¡¤ë§ íˆ´
{request.get('description', 'ì›¹ ë°ì´í„° ìˆ˜ì§‘ ìë™í™” ë„êµ¬')}
Version: {request.get('version', '1.0.0')}
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
from datetime import datetime
import os

class {request.get('class_name', 'CustomCrawler')}:
    """ë§ì¶¤í˜• í¬ë¡¤ëŸ¬ ì—”ì§„"""
    
    def __init__(self):
        self.name = "{request.get('project_name', 'Custom Crawler')}"
        self.version = "{request.get('version', '1.0.0')}"
        self.target_sites = {json.dumps(request.get('target_sites', []), ensure_ascii=False)}
        self.data_fields = {json.dumps(request.get('data_fields', {}), ensure_ascii=False)}
        self.results = []
        
    def crawl(self, url, selectors=None):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            # ì»¤ìŠ¤í…€ í—¤ë”
            headers = {{
                'User-Agent': '{request.get('user_agent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')}',
                'Accept-Language': '{request.get('language', 'ko-KR,ko;q=0.9,en;q=0.8')}'
            }}
            
            # ìš”ì²­ ì „ì†¡
            response = requests.get(url, headers=headers, timeout=30)
            response.encoding = 'utf-8'
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë°ì´í„° ì¶”ì¶œ
            if not selectors:
                selectors = self.data_fields
            
            extracted_data = {{}}
            for field, selector in selectors.items():
                try:
                    elements = soup.select(selector)
                    if elements:
                        if len(elements) == 1:
                            extracted_data[field] = elements[0].get_text(strip=True)
                        else:
                            extracted_data[field] = [el.get_text(strip=True) for el in elements[:50]]
                    else:
                        extracted_data[field] = ""
                except Exception as e:
                    extracted_data[field] = f"Error: {{str(e)}}"
            
            extracted_data['url'] = url
            extracted_data['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            
            self.results.append(extracted_data)
            return extracted_data
            
        except Exception as e:
            print(f"í¬ë¡¤ë§ ì˜¤ë¥˜: {{str(e)}}")
            return None
    
    def save_to_csv(self, filename=None):
        """CSVë¡œ ì €ì¥"""
        if not filename:
            filename = f"{{self.name}}_{{datetime.now().strftime('%Y%m%d_%H%M%S')}}.csv"
        
        df = pd.DataFrame(self.results)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        return filename
    
    def save_to_excel(self, filename=None):
        """Excelë¡œ ì €ì¥"""
        if not filename:
            filename = f"{{self.name}}_{{datetime.now().strftime('%Y%m%d_%H%M%S')}}.xlsx"
        
        df = pd.DataFrame(self.results)
        
        # ìŠ¤íƒ€ì¼ ì ìš©
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='í¬ë¡¤ë§ ê²°ê³¼')
            
            # ìŠ¤íƒ€ì¼ë§
            workbook = writer.book
            worksheet = writer.sheets['í¬ë¡¤ë§ ê²°ê³¼']
            
            # í—¤ë” ìŠ¤íƒ€ì¼
            from openpyxl.styles import PatternFill, Font, Alignment
            header_fill = PatternFill(start_color='{request.get('brand_color', '366092').replace('#', '')}', 
                                     end_color='{request.get('brand_color', '366092').replace('#', '')}', 
                                     fill_type='solid')
            header_font = Font(color='FFFFFF', bold=True)
            
            for cell in worksheet[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal='center')
            
            # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
            for column in worksheet.columns:
                max_length = 0
                column = [cell for cell in column]
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column[0].column_letter].width = adjusted_width
        
        return filename

# íŠ¹ìˆ˜ ê¸°ëŠ¥ ì¶”ê°€
{self._generate_special_features(request)}
'''
        
        # íŒŒì¼ ì €ì¥
        crawler_file = os.path.join(project_dir, f"{request.get('class_name', 'custom_crawler')}.py")
        with open(crawler_file, 'w', encoding='utf-8') as f:
            f.write(crawler_code)
    
    def _generate_special_features(self, request: Dict) -> str:
        """ê³ ê° ìš”ì²­ íŠ¹ìˆ˜ ê¸°ëŠ¥ ìƒì„±"""
        features = []
        
        # ë¡œê·¸ì¸ ê¸°ëŠ¥
        if request.get('needs_login', False):
            features.append('''
def login(self, username, password, login_url):
    """ë¡œê·¸ì¸ ê¸°ëŠ¥"""
    session = requests.Session()
    login_data = {
        'username': username,
        'password': password
    }
    session.post(login_url, data=login_data)
    return session
''')
        
        # í˜ì´ì§€ë„¤ì´ì…˜
        if request.get('needs_pagination', False):
            features.append('''
def crawl_multiple_pages(self, base_url, max_pages=10):
    """ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§"""
    for page in range(1, max_pages + 1):
        url = f"{base_url}?page={page}"
        self.crawl(url)
        time.sleep(self.delay)
''')
        
        # API í˜¸ì¶œ
        if request.get('needs_api', False):
            features.append('''
def call_api(self, api_url, params=None):
    """API í˜¸ì¶œ"""
    response = requests.get(api_url, params=params)
    return response.json()
''')
        
        # ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
        if request.get('needs_image_download', False):
            features.append('''
def download_images(self, img_urls, save_dir="images"):
    """ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ"""
    os.makedirs(save_dir, exist_ok=True)
    for i, url in enumerate(img_urls):
        response = requests.get(url)
        filename = os.path.join(save_dir, f"image_{i}.jpg")
        with open(filename, 'wb') as f:
            f.write(response.content)
''')
        
        return '\n'.join(features)
    
    def _generate_gui(self, project_dir: str, request: Dict):
        """GUI ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""
        
        gui_code = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
{request.get('project_name', 'Custom Crawler')} GUI
{request.get('company_name', 'ê³ ê°ì‚¬')} ì „ìš© ì¸í„°í˜ì´ìŠ¤
"""

import tkinter as tk
from tkinter import ttk, messagebox, filedialog, scrolledtext
import threading
from {request.get('class_name', 'custom_crawler')} import {request.get('class_name', 'CustomCrawler')}
import os
from datetime import datetime

class CrawlerGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("{request.get('project_name', 'Custom Crawler')} v{request.get('version', '1.0.0')}")
        self.root.geometry("{request.get('window_size', '900x700')}")
        
        # ë¸Œëœë“œ ìƒ‰ìƒ
        self.brand_color = "{request.get('brand_color', '#2196F3')}"
        
        # í¬ë¡¤ëŸ¬ ì¸ìŠ¤í„´ìŠ¤
        self.crawler = {request.get('class_name', 'CustomCrawler')}()
        
        self.create_widgets()
        
    def create_widgets(self):
        """UI ìƒì„±"""
        # í—¤ë”
        header = ttk.Frame(self.root)
        header.pack(fill=tk.X, padx=10, pady=5)
        
        title = ttk.Label(header, text="{request.get('project_name', 'Custom Crawler')}", 
                         font=('ë§‘ì€ ê³ ë”•', 16, 'bold'))
        title.pack(side=tk.LEFT)
        
        # URL ì…ë ¥
        input_frame = ttk.LabelFrame(self.root, text="í¬ë¡¤ë§ ì„¤ì •", padding=10)
        input_frame.pack(fill=tk.X, padx=10, pady=5)
        
        ttk.Label(input_frame, text="URL:").grid(row=0, column=0, sticky=tk.W)
        self.url_entry = ttk.Entry(input_frame, width=60)
        self.url_entry.grid(row=0, column=1, padx=5)
        self.url_entry.insert(0, "{request.get('default_url', '')}")
        
        # í”„ë¦¬ì…‹ ë²„íŠ¼ë“¤
        preset_frame = ttk.Frame(input_frame)
        preset_frame.grid(row=1, column=1, sticky=tk.W, pady=5)
        
        {self._generate_preset_buttons(request)}
        
        # ì„ íƒì ì…ë ¥
        selector_frame = ttk.LabelFrame(self.root, text="ë°ì´í„° í•„ë“œ", padding=10)
        selector_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        self.selector_text = scrolledtext.ScrolledText(selector_frame, height=8)
        self.selector_text.pack(fill=tk.BOTH, expand=True)
        
        # ê¸°ë³¸ ì„ íƒì
        default_selectors = """# {request.get('company_name', 'ê³ ê°ì‚¬')} ë§ì¶¤ ì„ íƒì
{self._generate_default_selectors(request)}"""
        self.selector_text.insert(1.0, default_selectors)
        
        # ì»¨íŠ¸ë¡¤ ë²„íŠ¼
        control_frame = ttk.Frame(self.root)
        control_frame.pack(pady=10)
        
        self.start_btn = ttk.Button(control_frame, text="ğŸš€ í¬ë¡¤ë§ ì‹œì‘", 
                                   command=self.start_crawling)
        self.start_btn.pack(side=tk.LEFT, padx=5)
        
        self.save_csv_btn = ttk.Button(control_frame, text="ğŸ’¾ CSV ì €ì¥", 
                                      command=self.save_csv)
        self.save_csv_btn.pack(side=tk.LEFT, padx=5)
        
        self.save_excel_btn = ttk.Button(control_frame, text="ğŸ“Š Excel ì €ì¥", 
                                        command=self.save_excel)
        self.save_excel_btn.pack(side=tk.LEFT, padx=5)
        
        # ë¡œê·¸
        log_frame = ttk.LabelFrame(self.root, text="ì‹¤í–‰ ë¡œê·¸", padding=5)
        log_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=5)
        
        self.log_text = scrolledtext.ScrolledText(log_frame, height=6)
        self.log_text.pack(fill=tk.BOTH, expand=True)
        
        # ìƒíƒœë°”
        self.status_bar = ttk.Label(self.root, text="ì¤€ë¹„ ì™„ë£Œ", relief=tk.SUNKEN)
        self.status_bar.pack(fill=tk.X)
    
    def start_crawling(self):
        """í¬ë¡¤ë§ ì‹œì‘"""
        url = self.url_entry.get()
        if not url:
            messagebox.showwarning("ê²½ê³ ", "URLì„ ì…ë ¥í•˜ì„¸ìš”.")
            return
        
        # ì„ íƒì íŒŒì‹±
        selectors = {{}}
        for line in self.selector_text.get(1.0, tk.END).split('\\n'):
            if ':' in line and not line.startswith('#'):
                key, value = line.split(':', 1)
                selectors[key.strip()] = value.strip()
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        self.log("í¬ë¡¤ë§ ì‹œì‘: " + url)
        result = self.crawler.crawl(url, selectors)
        
        if result:
            self.log(f"âœ… ì„±ê³µ: {{len(result)}} í•„ë“œ ìˆ˜ì§‘")
            self.status_bar.config(text=f"í¬ë¡¤ë§ ì™„ë£Œ - {{len(self.crawler.results)}} í•­ëª©")
        else:
            self.log("âŒ í¬ë¡¤ë§ ì‹¤íŒ¨")
    
    def save_csv(self):
        """CSV ì €ì¥"""
        if not self.crawler.results:
            messagebox.showwarning("ê²½ê³ ", "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        filename = filedialog.asksaveasfilename(
            defaultextension=".csv",
            filetypes=[("CSV íŒŒì¼", "*.csv")],
            initialfile=f"{{self.crawler.name}}_{{datetime.now().strftime('%Y%m%d')}}.csv"
        )
        
        if filename:
            self.crawler.save_to_csv(filename)
            self.log(f"ğŸ’¾ CSV ì €ì¥ ì™„ë£Œ: {{filename}}")
            messagebox.showinfo("ì„±ê³µ", "CSV íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def save_excel(self):
        """Excel ì €ì¥"""
        if not self.crawler.results:
            messagebox.showwarning("ê²½ê³ ", "ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        filename = filedialog.asksaveasfilename(
            defaultextension=".xlsx",
            filetypes=[("Excel íŒŒì¼", "*.xlsx")],
            initialfile=f"{{self.crawler.name}}_{{datetime.now().strftime('%Y%m%d')}}.xlsx"
        )
        
        if filename:
            self.crawler.save_to_excel(filename)
            self.log(f"ğŸ“Š Excel ì €ì¥ ì™„ë£Œ: {{filename}}")
            messagebox.showinfo("ì„±ê³µ", "Excel íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def log(self, message):
        """ë¡œê·¸ ì¶œë ¥"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        self.log_text.insert(tk.END, f"[{{timestamp}}] {{message}}\\n")
        self.log_text.see(tk.END)
    
    {self._generate_preset_methods(request)}

def main():
    root = tk.Tk()
    app = CrawlerGUI(root)
    root.mainloop()

if __name__ == "__main__":
    main()
'''
        
        # GUI íŒŒì¼ ì €ì¥
        gui_file = os.path.join(project_dir, "gui.py")
        with open(gui_file, 'w', encoding='utf-8') as f:
            f.write(gui_code)
    
    def _generate_preset_buttons(self, request: Dict) -> str:
        """í”„ë¦¬ì…‹ ë²„íŠ¼ ìƒì„±"""
        buttons = []
        for i, preset in enumerate(request.get('presets', [])):
            buttons.append(f'''
        ttk.Button(preset_frame, text="{preset['name']}", 
                  command=self.preset_{i}).pack(side=tk.LEFT, padx=2)''')
        return ''.join(buttons)
    
    def _generate_preset_methods(self, request: Dict) -> str:
        """í”„ë¦¬ì…‹ ë©”ì„œë“œ ìƒì„±"""
        methods = []
        for i, preset in enumerate(request.get('presets', [])):
            methods.append(f'''
    def preset_{i}(self):
        """í”„ë¦¬ì…‹: {preset['name']}"""
        self.url_entry.delete(0, tk.END)
        self.url_entry.insert(0, "{preset.get('url', '')}")
        self.selector_text.delete(1.0, tk.END)
        selectors = """{preset.get('selectors', '')}"""
        self.selector_text.insert(1.0, selectors)
        self.log("í”„ë¦¬ì…‹ ì ìš©: {preset['name']}")
''')
        return ''.join(methods)
    
    def _generate_default_selectors(self, request: Dict) -> str:
        """ê¸°ë³¸ ì„ íƒì ìƒì„±"""
        selectors = []
        for field, selector in request.get('data_fields', {}).items():
            selectors.append(f"{field}: {selector}")
        return '\n'.join(selectors)
    
    def _generate_presets(self, project_dir: str, request: Dict):
        """í”„ë¦¬ì…‹ ì„¤ì • íŒŒì¼ ìƒì„±"""
        presets = {
            "version": request.get('version', '1.0.0'),
            "presets": request.get('presets', []),
            "default_settings": {
                "delay": request.get('delay', 0.5),
                "max_pages": request.get('max_pages', 10),
                "timeout": request.get('timeout', 30)
            }
        }
        
        preset_file = os.path.join(project_dir, "presets.json")
        with open(preset_file, 'w', encoding='utf-8') as f:
            json.dump(presets, f, ensure_ascii=False, indent=2)
    
    def _generate_build_scripts(self, project_dir: str, request: Dict):
        """ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        
        # requirements.txt
        requirements = f"""# {request.get('project_name', 'Custom Crawler')} Dependencies
requests==2.31.0
beautifulsoup4==4.12.2
pandas==2.2.3
openpyxl==3.1.2
lxml==5.1.0
pyinstaller==6.3.0
"""
        if request.get('needs_selenium', False):
            requirements += "selenium==4.16.0\nwebdriver-manager==4.0.1\n"
        
        req_file = os.path.join(project_dir, "requirements.txt")
        with open(req_file, 'w', encoding='utf-8') as f:
            f.write(requirements)
        
        # build.py
        build_script = f'''#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸ - {request.get('project_name', 'Custom Crawler')}"""

import PyInstaller.__main__
import os
import shutil

def build():
    """EXE ë¹Œë“œ"""
    # ì´ì „ ë¹Œë“œ ì •ë¦¬
    for dir in ['build', 'dist', '__pycache__']:
        if os.path.exists(dir):
            shutil.rmtree(dir)
    
    # PyInstaller ì‹¤í–‰
    PyInstaller.__main__.run([
        'gui.py',
        '--name', '{request.get('exe_name', 'CustomCrawler')}',
        '--onefile',
        '--windowed',
        '--icon', 'icon.ico',
        '--add-data', 'presets.json;.',
        '--hidden-import', 'tkinter',
        '--hidden-import', 'requests',
        '--hidden-import', 'bs4',
        '--hidden-import', 'pandas',
        '--hidden-import', 'openpyxl',
        '--clean',
        '--noconfirm'
    ])
    
    print("âœ… ë¹Œë“œ ì™„ë£Œ!")
    print(f"ì‹¤í–‰ íŒŒì¼: dist/{request.get('exe_name', 'CustomCrawler')}.exe")

if __name__ == "__main__":
    build()
'''
        
        build_file = os.path.join(project_dir, "build.py")
        with open(build_file, 'w', encoding='utf-8') as f:
            f.write(build_script)
        
        # build.bat
        batch_script = f"""@echo off
echo ========================================
echo {request.get('project_name', 'Custom Crawler')} ë¹Œë“œ
echo ========================================
echo.

REM ê°€ìƒí™˜ê²½ ìƒì„±
if not exist "venv" (
    python -m venv venv
)

REM ê°€ìƒí™˜ê²½ í™œì„±í™”
call venv\\Scripts\\activate.bat

REM ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

REM ë¹Œë“œ ì‹¤í–‰
python build.py

echo.
echo ë¹Œë“œ ì™„ë£Œ!
pause
"""
        
        batch_file = os.path.join(project_dir, "build.bat")
        with open(batch_file, 'w', encoding='utf-8') as f:
            f.write(batch_script)
    
    def _generate_installer(self, project_dir: str, request: Dict):
        """Inno Setup ì¸ìŠ¤í†¨ëŸ¬ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
        
        installer_script = f'''; Inno Setup Script for {request.get('project_name', 'Custom Crawler')}

#define MyAppName "{request.get('project_name', 'Custom Crawler')}"
#define MyAppVersion "{request.get('version', '1.0.0')}"
#define MyAppPublisher "{request.get('company_name', 'ê³ ê°ì‚¬')}"
#define MyAppExeName "{request.get('exe_name', 'CustomCrawler')}.exe"

[Setup]
AppId={{{{request.get('app_id', 'E8F4B3C2-9A7D-4F2E-B5C8-1D3A6E9F8B2C')}}}}
AppName={{#MyAppName}}
AppVersion={{#MyAppVersion}}
AppPublisher={{#MyAppPublisher}}
DefaultDirName={{autopf}}\\{{#MyAppName}}
DefaultGroupName={{#MyAppName}}
OutputDir=installer
OutputBaseFilename={{#MyAppName}}_Setup_v{{#MyAppVersion}}
SetupIconFile=icon.ico
Compression=lzma2
SolidCompression=yes
WizardStyle=modern

[Languages]
Name: "korean"; MessagesFile: "compiler:Languages\\Korean.isl"

[Tasks]
Name: "desktopicon"; Description: "{{cm:CreateDesktopIcon}}"; GroupDescription: "{{cm:AdditionalIcons}}"

[Files]
Source: "dist\\{{#MyAppExeName}}"; DestDir: "{{app}}"; Flags: ignoreversion
Source: "README.md"; DestDir: "{{app}}"; Flags: ignoreversion

[Icons]
Name: "{{group}}\\{{#MyAppName}}"; Filename: "{{app}}\\{{#MyAppExeName}}"
Name: "{{autodesktop}}\\{{#MyAppName}}"; Filename: "{{app}}\\{{#MyAppExeName}}"; Tasks: desktopicon

[Run]
Filename: "{{app}}\\{{#MyAppExeName}}"; Description: "{{cm:LaunchProgram,{{#StringChange(MyAppName, '&', '&&')}}}}"; Flags: nowait postinstall skipifsilent
'''
        
        installer_file = os.path.join(project_dir, "installer.iss")
        with open(installer_file, 'w', encoding='utf-8') as f:
            f.write(installer_script)
    
    def _generate_documentation(self, project_dir: str, request: Dict):
        """ì‚¬ìš© ì„¤ëª…ì„œ ìƒì„±"""
        
        readme = f"""# {request.get('project_name', 'Custom Crawler')}

## ì†Œê°œ
{request.get('company_name', 'ê³ ê°ì‚¬')} ì „ìš© í¬ë¡¤ë§ íˆ´
{request.get('description', 'ì›¹ ë°ì´í„° ìë™ ìˆ˜ì§‘ ë„êµ¬')}

## ë²„ì „
v{request.get('version', '1.0.0')}

## ì£¼ìš” ê¸°ëŠ¥
{self._generate_feature_list(request)}

## ì‚¬ìš©ë²•

### 1. í”„ë¡œê·¸ë¨ ì‹¤í–‰
- ë°”íƒ•í™”ë©´ì˜ "{request.get('project_name', 'Custom Crawler')}" ì•„ì´ì½˜ ë”ë¸”í´ë¦­
- ë˜ëŠ” ì‹œì‘ ë©”ë‰´ì—ì„œ ì‹¤í–‰

### 2. URL ì…ë ¥
- í¬ë¡¤ë§í•  ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ ì…ë ¥
{f"- ê¸°ë³¸ URL: {request.get('default_url', '')}" if request.get('default_url') else ""}

### 3. ë°ì´í„° í•„ë“œ ì„¤ì •
- í”„ë¦¬ì…‹ ë²„íŠ¼ í´ë¦­ ë˜ëŠ” ì§ì ‘ ì…ë ¥
- í˜•ì‹: í•„ë“œëª…: CSSì„ íƒì

### 4. í¬ë¡¤ë§ ì‹¤í–‰
- [ğŸš€ í¬ë¡¤ë§ ì‹œì‘] ë²„íŠ¼ í´ë¦­
- ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸

### 5. ê²°ê³¼ ì €ì¥
- [ğŸ’¾ CSV ì €ì¥] - CSV í˜•ì‹ìœ¼ë¡œ ì €ì¥
- [ğŸ“Š Excel ì €ì¥] - Excel í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ê¶Œì¥)

## í”„ë¦¬ì…‹ ëª©ë¡
{self._generate_preset_docs(request)}

## ì§€ì› ì‚¬ì´íŠ¸
{chr(10).join(['- ' + site for site in request.get('target_sites', [])])}

## ë¬¸ì˜ì²˜
- ë‹´ë‹¹ì: {request.get('contact_name', 'ë‹´ë‹¹ì')}
- ì´ë©”ì¼: {request.get('contact_email', 'support@example.com')}
- ì „í™”: {request.get('contact_phone', '02-1234-5678')}

## ë¼ì´ì„¼ìŠ¤
{request.get('license', 'Â© 2024 All rights reserved.')}
"""
        
        readme_file = os.path.join(project_dir, "README.md")
        with open(readme_file, 'w', encoding='utf-8') as f:
            f.write(readme)
    
    def _generate_feature_list(self, request: Dict) -> str:
        """ê¸°ëŠ¥ ëª©ë¡ ìƒì„±"""
        features = ["- âœ… ì‹¤ì‹œê°„ ì›¹ í¬ë¡¤ë§", "- ğŸ“Š CSV/Excel ì €ì¥", "- ğŸ¯ ë§ì¶¤í˜• í”„ë¦¬ì…‹"]
        
        if request.get('needs_login'):
            features.append("- ğŸ” ë¡œê·¸ì¸ ì§€ì›")
        if request.get('needs_pagination'):
            features.append("- ğŸ“„ í˜ì´ì§€ë„¤ì´ì…˜")
        if request.get('needs_api'):
            features.append("- ğŸ”Œ API ì—°ë™")
        if request.get('needs_image_download'):
            features.append("- ğŸ–¼ï¸ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ")
        if request.get('needs_selenium'):
            features.append("- ğŸŒ ë™ì  í˜ì´ì§€ ì§€ì›")
        
        return '\n'.join(features)
    
    def _generate_preset_docs(self, request: Dict) -> str:
        """í”„ë¦¬ì…‹ ë¬¸ì„œ ìƒì„±"""
        docs = []
        for preset in request.get('presets', []):
            docs.append(f"### {preset['name']}")
            docs.append(f"- URL: {preset.get('url', '')}")
            docs.append(f"- ìš©ë„: {preset.get('description', '')}")
            docs.append("")
        return '\n'.join(docs)
    
    def _generate_resources(self, project_dir: str, request: Dict):
        """ë¦¬ì†ŒìŠ¤ íŒŒì¼ ìƒì„±"""
        
        # ì•„ì´ì½˜ (ì„ì‹œ)
        icon_file = os.path.join(project_dir, "icon.ico")
        with open(icon_file, 'w', encoding='utf-8') as f:
            f.write(request.get('icon_emoji', 'ğŸ•·ï¸'))
        
        # ì„¤ì • íŒŒì¼
        config = {
            "app_name": request.get('project_name', 'Custom Crawler'),
            "version": request.get('version', '1.0.0'),
            "company": request.get('company_name', 'ê³ ê°ì‚¬'),
            "settings": {
                "default_delay": request.get('delay', 0.5),
                "max_pages": request.get('max_pages', 10),
                "timeout": request.get('timeout', 30),
                "user_agent": request.get('user_agent', 'Mozilla/5.0')
            }
        }
        
        config_file = os.path.join(project_dir, "config.json")
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)


# ì‚¬ìš© ì˜ˆì‹œ
def create_crawler_from_request(customer_request: Dict):
    """ê³ ê° ìš”ì²­ì‚¬í•­ìœ¼ë¡œ í¬ë¡¤ëŸ¬ ìƒì„±"""
    
    factory = CrawlerFactory()
    project_path = factory.create_custom_crawler(customer_request)
    
    print(f"""
    âœ¨ í¬ë¡¤ëŸ¬ ìƒì„± ì™„ë£Œ!
    ===========================
    í”„ë¡œì íŠ¸ ê²½ë¡œ: {project_path}
    
    ë‹¤ìŒ ë‹¨ê³„:
    1. cd {project_path}
    2. build.bat ì‹¤í–‰ (Windows)
    3. installer/ í´ë”ì—ì„œ ì„¤ì¹˜ íŒŒì¼ í™•ì¸
    
    ìƒì„±ëœ íŒŒì¼:
    - gui.py: ë©”ì¸ GUI ì• í”Œë¦¬ì¼€ì´ì…˜
    - {customer_request.get('class_name', 'custom_crawler')}.py: í¬ë¡¤ëŸ¬ ì—”ì§„
    - build.bat: ë¹Œë“œ ìŠ¤í¬ë¦½íŠ¸
    - installer.iss: ì¸ìŠ¤í†¨ëŸ¬ ìŠ¤í¬ë¦½íŠ¸
    - README.md: ì‚¬ìš© ì„¤ëª…ì„œ
    """)
    
    return project_path


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ê³ ê° ìš”ì²­ì‚¬í•­
    sample_request = {
        "project_name": "ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬",
        "company_name": "ABC ê¸°ì—…",
        "version": "1.0.0",
        "description": "ë„¤ì´ë²„ ë‰´ìŠ¤ ìë™ ìˆ˜ì§‘ ë„êµ¬",
        "exe_name": "NaverNewsCrawler",
        "class_name": "NaverCrawler",
        "window_size": "900x700",
        "brand_color": "#03C75A",  # ë„¤ì´ë²„ ê·¸ë¦°
        
        "target_sites": [
            "https://news.naver.com",
            "https://news.naver.com/section/105"
        ],
        
        "default_url": "https://news.naver.com/section/105",
        
        "data_fields": {
            "title": ".sa_text_title",
            "content": ".sa_text_lede", 
            "date": ".sa_text_datetime",
            "author": ".sa_text_press",
            "link": ".sa_text a[href]"
        },
        
        "presets": [
            {
                "name": "IT ë‰´ìŠ¤",
                "url": "https://news.naver.com/section/105",
                "description": "IT/ê³¼í•™ ë‰´ìŠ¤",
                "selectors": "title: .sa_text_title\\ncontent: .sa_text_lede\\ndate: .sa_text_datetime"
            },
            {
                "name": "ê²½ì œ ë‰´ìŠ¤",
                "url": "https://news.naver.com/section/101",
                "description": "ê²½ì œ ë‰´ìŠ¤",
                "selectors": "title: .sa_text_title\\ncontent: .sa_text_lede\\ndate: .sa_text_datetime"
            }
        ],
        
        "needs_pagination": True,
        "needs_image_download": False,
        "needs_login": False,
        "needs_api": False,
        "needs_selenium": False,
        
        "delay": 0.5,
        "max_pages": 10,
        "timeout": 30,
        
        "contact_name": "í™ê¸¸ë™",
        "contact_email": "support@abc.com",
        "contact_phone": "02-1234-5678",
        
        "license": "Â© 2024 ABC ê¸°ì—…. All rights reserved.",
        "icon_emoji": "ğŸ“°"
    }
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    create_crawler_from_request(sample_request)