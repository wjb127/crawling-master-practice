"""
í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ ì›¹ ì¸í„°í˜ì´ìŠ¤
ê³ ê° ìš”êµ¬ì‚¬í•­ì„ ì…ë ¥ë°›ì•„ ìë™ìœ¼ë¡œ í¬ë¡¤ëŸ¬ë¥¼ ìƒì„±í•˜ëŠ” ì›¹ ì„œë¹„ìŠ¤
"""

from fastapi import FastAPI, Request, BackgroundTasks
from fastapi.responses import HTMLResponse, FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import os
import json
import shutil
import tempfile
import asyncio
from datetime import datetime
from typing import Dict, Any, List
from pathlib import Path
import zipfile

# Factory system import
from factory_system import CrawlerFactory

app = FastAPI(title="í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬", version="1.0.0")

# Templates setup
templates_dir = Path(__file__).parent / "templates"
templates_dir.mkdir(exist_ok=True)
templates = Jinja2Templates(directory=str(templates_dir))

# Static files
static_dir = Path(__file__).parent / "static"
static_dir.mkdir(exist_ok=True)
app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# Storage for generation jobs
generation_jobs: Dict[str, Dict[str, Any]] = {}

# Factory instance
factory = CrawlerFactory()

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€ - í¬ë¡¤ëŸ¬ ìƒì„± í¼"""
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/api/generate")
async def generate_crawler(
    request: Request,
    background_tasks: BackgroundTasks
):
    """í¬ë¡¤ëŸ¬ ìƒì„± ìš”ì²­ ì²˜ë¦¬"""
    form_data = await request.json()
    
    # Generate unique job ID
    job_id = f"job_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # Parse customer requirements
    customer_request = {
        "project_name": form_data.get("project_name", "CustomCrawler"),
        "company_name": form_data.get("company_name", "Customer Company"),
        "target_sites": form_data.get("target_sites", []),
        "data_fields": form_data.get("data_fields", []),
        "output_format": form_data.get("output_format", "excel"),
        "crawling_method": form_data.get("crawling_method", "static"),
        "login_required": form_data.get("login_required", False),
        "schedule_required": form_data.get("schedule_required", False),
        "proxy_support": form_data.get("proxy_support", False),
        "multi_threading": form_data.get("multi_threading", True),
        "error_retry": form_data.get("error_retry", True),
        "custom_features": form_data.get("custom_features", [])
    }
    
    # Store job info
    generation_jobs[job_id] = {
        "status": "processing",
        "progress": 0,
        "created_at": datetime.now().isoformat(),
        "request": customer_request,
        "output_path": None,
        "error": None
    }
    
    # Start background generation
    background_tasks.add_task(
        generate_crawler_background,
        job_id,
        customer_request
    )
    
    return JSONResponse({
        "job_id": job_id,
        "status": "started",
        "message": "í¬ë¡¤ëŸ¬ ìƒì„±ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤."
    })

async def generate_crawler_background(job_id: str, customer_request: Dict[str, Any]):
    """ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ëŸ¬ ìƒì„±"""
    try:
        # Update progress
        generation_jobs[job_id]["progress"] = 10
        generation_jobs[job_id]["status"] = "generating"
        
        # Create output directory
        output_dir = Path(tempfile.mkdtemp()) / customer_request["project_name"]
        
        # Generate crawler using factory
        generation_jobs[job_id]["progress"] = 30
        project_path = factory.create_custom_crawler(customer_request)
        
        # Create installer if path exists
        generation_jobs[job_id]["progress"] = 60
        if project_path and Path(project_path).exists():
            # Build executable
            generation_jobs[job_id]["progress"] = 70
            factory.build_executable(project_path)
            
            # Create installer
            generation_jobs[job_id]["progress"] = 85
            installer_path = factory.create_installer(project_path)
            
            # Create zip package
            generation_jobs[job_id]["progress"] = 95
            zip_path = Path(project_path).parent / f"{customer_request['project_name']}_package.zip"
            
            with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Add all project files
                for root, dirs, files in os.walk(project_path):
                    for file in files:
                        file_path = Path(root) / file
                        arcname = file_path.relative_to(Path(project_path).parent)
                        zipf.write(file_path, arcname)
                
                # Add installer if exists
                if installer_path and Path(installer_path).exists():
                    zipf.write(installer_path, Path(installer_path).name)
            
            generation_jobs[job_id]["output_path"] = str(zip_path)
            generation_jobs[job_id]["installer_path"] = str(installer_path) if installer_path else None
        
        # Complete
        generation_jobs[job_id]["progress"] = 100
        generation_jobs[job_id]["status"] = "completed"
        
    except Exception as e:
        generation_jobs[job_id]["status"] = "failed"
        generation_jobs[job_id]["error"] = str(e)

@app.get("/api/status/{job_id}")
async def get_job_status(job_id: str):
    """ìƒì„± ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    if job_id not in generation_jobs:
        return JSONResponse(
            {"error": "Job not found"},
            status_code=404
        )
    
    job = generation_jobs[job_id]
    return JSONResponse({
        "job_id": job_id,
        "status": job["status"],
        "progress": job["progress"],
        "created_at": job["created_at"],
        "error": job["error"]
    })

@app.get("/api/download/{job_id}")
async def download_crawler(job_id: str):
    """ìƒì„±ëœ í¬ë¡¤ëŸ¬ ë‹¤ìš´ë¡œë“œ"""
    if job_id not in generation_jobs:
        return JSONResponse(
            {"error": "Job not found"},
            status_code=404
        )
    
    job = generation_jobs[job_id]
    if job["status"] != "completed":
        return JSONResponse(
            {"error": "Job not completed"},
            status_code=400
        )
    
    if not job["output_path"] or not Path(job["output_path"]).exists():
        return JSONResponse(
            {"error": "Output file not found"},
            status_code=404
        )
    
    return FileResponse(
        job["output_path"],
        media_type="application/zip",
        filename=f"{job['request']['project_name']}_package.zip"
    )

@app.get("/api/presets")
async def get_presets():
    """ì‚¬ì „ ì •ì˜ëœ í”„ë¦¬ì…‹ ëª©ë¡"""
    presets = [
        {
            "id": "ecommerce",
            "name": "ì´ì»¤ë¨¸ìŠ¤ ìƒí’ˆ í¬ë¡¤ëŸ¬",
            "description": "ì‡¼í•‘ëª° ìƒí’ˆ ì •ë³´ ìˆ˜ì§‘",
            "data_fields": ["ìƒí’ˆëª…", "ê°€ê²©", "í• ì¸ìœ¨", "ë¦¬ë·°ìˆ˜", "í‰ì ", "ì´ë¯¸ì§€URL"],
            "features": ["ê°€ê²© ì¶”ì ", "ì¬ê³  í™•ì¸", "ë¦¬ë·° ë¶„ì„"]
        },
        {
            "id": "news",
            "name": "ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ëŸ¬",
            "description": "ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ê¸°ì‚¬ ìˆ˜ì§‘",
            "data_fields": ["ì œëª©", "ë³¸ë¬¸", "ì‘ì„±ì", "ë‚ ì§œ", "ì¹´í…Œê³ ë¦¬", "ì¡°íšŒìˆ˜"],
            "features": ["í‚¤ì›Œë“œ í•„í„°ë§", "ì¤‘ë³µ ì œê±°", "ìš”ì•½ ìƒì„±"]
        },
        {
            "id": "social",
            "name": "ì†Œì…œ ë¯¸ë””ì–´ í¬ë¡¤ëŸ¬",
            "description": "SNS ê²Œì‹œë¬¼ ìˆ˜ì§‘",
            "data_fields": ["ì‘ì„±ì", "ë‚´ìš©", "ì¢‹ì•„ìš”ìˆ˜", "ëŒ“ê¸€ìˆ˜", "í•´ì‹œíƒœê·¸", "ì‘ì„±ì¼"],
            "features": ["ì¸í”Œë£¨ì–¸ì„œ ì¶”ì ", "íŠ¸ë Œë“œ ë¶„ì„", "ê°ì„± ë¶„ì„"]
        },
        {
            "id": "realestate",
            "name": "ë¶€ë™ì‚° ë§¤ë¬¼ í¬ë¡¤ëŸ¬",
            "description": "ë¶€ë™ì‚° ì‚¬ì´íŠ¸ ë§¤ë¬¼ ì •ë³´",
            "data_fields": ["ë§¤ë¬¼ìœ í˜•", "ê°€ê²©", "ë©´ì ", "ìœ„ì¹˜", "ì¸µìˆ˜", "ì—°ë½ì²˜"],
            "features": ["ê°€ê²© ë³€ë™ ì¶”ì ", "ì§€ì—­ë³„ ë¶„ë¥˜", "ì•Œë¦¼ ê¸°ëŠ¥"]
        },
        {
            "id": "job",
            "name": "ì±„ìš©ê³µê³  í¬ë¡¤ëŸ¬",
            "description": "êµ¬ì¸êµ¬ì§ ì‚¬ì´íŠ¸ ì±„ìš©ì •ë³´",
            "data_fields": ["íšŒì‚¬ëª…", "ì§ë¬´", "ì—°ë´‰", "ê²½ë ¥", "ë§ˆê°ì¼", "ë³µì§€"],
            "features": ["í‚¤ì›Œë“œ ë§¤ì¹­", "ì—°ë´‰ í†µê³„", "ê¸°ì—… ì •ë³´"]
        }
    ]
    return JSONResponse(presets)

@app.get("/api/examples")
async def get_examples():
    """ì˜ˆì œ ìš”êµ¬ì‚¬í•­"""
    examples = [
        {
            "title": "ë„¤ì´ë²„ ì‡¼í•‘ ë² ìŠ¤íŠ¸100",
            "description": "ë„¤ì´ë²„ ì‡¼í•‘ ì¹´í…Œê³ ë¦¬ë³„ ë² ìŠ¤íŠ¸ ìƒí’ˆ 100ê°œ ìˆ˜ì§‘",
            "requirements": {
                "target_sites": ["https://shopping.naver.com"],
                "data_fields": ["ìˆœìœ„", "ìƒí’ˆëª…", "ê°€ê²©", "í• ì¸ìœ¨", "ë¦¬ë·°ìˆ˜"],
                "output_format": "excel",
                "schedule_required": True
            }
        },
        {
            "title": "ë¶€ë™ì‚° ì‹œì„¸ ëª¨ë‹ˆí„°ë§",
            "description": "íŠ¹ì • ì§€ì—­ ì•„íŒŒíŠ¸ ë§¤ë§¤/ì „ì„¸ ì‹œì„¸ ì¼ì¼ ìˆ˜ì§‘",
            "requirements": {
                "target_sites": ["ì§ë°©", "ë‹¤ë°©", "ë„¤ì´ë²„ë¶€ë™ì‚°"],
                "data_fields": ["ë‹¨ì§€ëª…", "ë©´ì ", "ë§¤ë§¤ê°€", "ì „ì„¸ê°€", "ê±°ë˜ì¼"],
                "output_format": "database",
                "schedule_required": True
            }
        },
        {
            "title": "ê²½ìŸì‚¬ ê°€ê²© ëª¨ë‹ˆí„°ë§",
            "description": "ê²½ìŸì‚¬ ì œí’ˆ ê°€ê²© ë³€ë™ ì‹¤ì‹œê°„ ì¶”ì ",
            "requirements": {
                "target_sites": ["ì¿ íŒ¡", "11ë²ˆê°€", "Gë§ˆì¼“"],
                "data_fields": ["ìƒí’ˆì½”ë“œ", "ìƒí’ˆëª…", "íŒë§¤ê°€", "í• ì¸ê°€", "ì¬ê³ "],
                "output_format": "api",
                "schedule_required": True,
                "custom_features": ["ê°€ê²© ë³€ë™ ì•Œë¦¼", "ì¬ê³  ì†Œì§„ ì•Œë¦¼"]
            }
        }
    ]
    return JSONResponse(examples)

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸ­ í¬ë¡¤ëŸ¬ íŒ©í† ë¦¬ ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹œì‘...")
    print("ğŸ“ http://localhost:8001 ì—ì„œ ì ‘ì† ê°€ëŠ¥í•©ë‹ˆë‹¤")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8001,
        reload=True
    )