#!/usr/bin/env python3
"""
ì‹¬í”Œí•œ FastAPI + HTMX í¬ë¡¤ë§ ì„œë¹„ìŠ¤
DB ì—†ì´ ë©”ëª¨ë¦¬ ì €ì¥ + ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
"""

from fastapi import FastAPI, Request, Form, BackgroundTasks
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import pandas as pd
import json
import uuid
from datetime import datetime
from typing import Optional, Dict, List, Any
from enum import Enum
from pathlib import Path
from urllib.parse import urlparse, urljoin
import openpyxl
from openpyxl.styles import PatternFill, Font, Alignment
from io import BytesIO


# ==================== ì„¤ì • ====================
class CrawlStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


class CrawlJob:
    def __init__(self, name: str, url: str, selectors: Dict[str, str]):
        self.id = str(uuid.uuid4())[:8]  # ì§§ì€ ID
        self.name = name
        self.url = url
        self.selectors = selectors
        self.status = CrawlStatus.PENDING
        self.progress = 0
        self.total_items = 0
        self.collected_items = 0
        self.created_at = datetime.now()
        self.result_file = None
        self.data = []  # ìˆ˜ì§‘ëœ ë°ì´í„°
        self.logs = []


# ==================== ì•± ì´ˆê¸°í™” ====================
app = FastAPI(title="Simple Crawler")
templates = Jinja2Templates(directory="templates")

# ì „ì—­ ì €ì¥ì†Œ (ë©”ëª¨ë¦¬)
jobs_store: Dict[str, CrawlJob] = {}

# ë””ë ‰í† ë¦¬ ìƒì„±
Path("templates").mkdir(exist_ok=True)
Path("downloads").mkdir(exist_ok=True)
app.mount("/downloads", StaticFiles(directory="downloads"), name="downloads")


# ==================== í¬ë¡¤ëŸ¬ ì—”ì§„ ====================
class SimpleCrawler:
    """ì‹¬í”Œí•œ ë¹„ë™ê¸° í¬ë¡¤ëŸ¬"""
    
    def __init__(self, job: CrawlJob):
        self.job = job
        self.session = None
        
    async def crawl(self):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            self.job.status = CrawlStatus.RUNNING
            self.log(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {self.job.url}")
            
            async with aiohttp.ClientSession() as self.session:
                # ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
                html = await self.fetch_page(self.job.url)
                if not html:
                    raise Exception("í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
                # ë°ì´í„° ì¶”ì¶œ
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì„ íƒìë³„ë¡œ ë°ì´í„° ì¶”ì¶œ
                main_data = {}
                for field, selector in self.job.selectors.items():
                    try:
                        elements = soup.select(selector)
                        if elements:
                            # ì—¬ëŸ¬ ê°œë©´ ë¦¬ìŠ¤íŠ¸ë¡œ, í•˜ë‚˜ë©´ í…ìŠ¤íŠ¸ë¡œ
                            if len(elements) > 1:
                                main_data[field] = [el.get_text(strip=True) for el in elements[:10]]
                            else:
                                main_data[field] = elements[0].get_text(strip=True)
                        else:
                            main_data[field] = ""
                    except Exception as e:
                        main_data[field] = f"Error: {str(e)}"
                        self.log(f"âš ï¸ {field} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                
                main_data['url'] = self.job.url
                main_data['crawled_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                self.job.data.append(main_data)
                
                # ì¶”ê°€ í˜ì´ì§€ ì°¾ê¸° (ì˜µì…˜)
                links = self.find_similar_links(soup, self.job.url)
                self.job.total_items = len(links) + 1
                
                # ìµœëŒ€ 10ê°œ ì¶”ê°€ í˜ì´ì§€ë§Œ í¬ë¡¤ë§
                for idx, link in enumerate(links[:10], 1):
                    self.job.progress = int((idx / min(len(links) + 1, 11)) * 100)
                    self.log(f"ğŸ“„ í˜ì´ì§€ {idx}/{ min(len(links), 10)} í¬ë¡¤ë§ ì¤‘...")
                    
                    page_html = await self.fetch_page(link)
                    if page_html:
                        page_soup = BeautifulSoup(page_html, 'html.parser')
                        page_data = {}
                        
                        for field, selector in self.job.selectors.items():
                            try:
                                elements = page_soup.select(selector)
                                if elements:
                                    page_data[field] = elements[0].get_text(strip=True)
                                else:
                                    page_data[field] = ""
                            except:
                                page_data[field] = ""
                        
                        page_data['url'] = link
                        page_data['crawled_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        self.job.data.append(page_data)
                        self.job.collected_items += 1
                    
                    await asyncio.sleep(0.5)  # Rate limiting
                
                # ì—‘ì…€ íŒŒì¼ ìƒì„±
                if self.job.data:
                    self.save_to_excel()
                
                self.job.status = CrawlStatus.COMPLETED
                self.job.progress = 100
                self.log(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! {len(self.job.data)}ê°œ í•­ëª© ìˆ˜ì§‘")
                
        except Exception as e:
            self.job.status = CrawlStatus.FAILED
            self.log(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
            }
            async with self.session.get(url, headers=headers, timeout=10, ssl=False) as response:
                if response.status == 200:
                    return await response.text()
        except Exception as e:
            self.log(f"í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {str(e)}")
        return None
    
    def find_similar_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """ë¹„ìŠ·í•œ íŒ¨í„´ì˜ ë§í¬ ì°¾ê¸°"""
        links = set()
        base_domain = urlparse(base_url).netloc
        
        for a in soup.find_all('a', href=True)[:50]:  # ìµœëŒ€ 50ê°œë§Œ í™•ì¸
            href = a['href']
            full_url = urljoin(base_url, href)
            
            # ê°™ì€ ë„ë©”ì¸ì´ê³ , íŠ¹ì • íŒ¨í„´ì´ ìˆìœ¼ë©´ ì¶”ê°€
            if urlparse(full_url).netloc == base_domain:
                # ë‰´ìŠ¤, ìƒí’ˆ, ê²Œì‹œê¸€ ë“±ì˜ íŒ¨í„´
                if any(pattern in full_url for pattern in ['/article/', '/product/', '/post/', '/item/', '/news/']):
                    links.add(full_url)
        
        return list(links)[:20]  # ìµœëŒ€ 20ê°œ
    
    def save_to_excel(self):
        """ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥ (ìŠ¤íƒ€ì¼ í¬í•¨)"""
        try:
            df = pd.DataFrame(self.job.data)
            
            # íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"{self.job.name.replace(' ', '_')}_{timestamp}"
            filepath = f"downloads/{filename}.xlsx"
            
            # ì—‘ì…€ Writer ìƒì„±
            with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='í¬ë¡¤ë§ ê²°ê³¼', index=False)
                
                # ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸°
                worksheet = writer.sheets['í¬ë¡¤ë§ ê²°ê³¼']
                
                # í—¤ë” ìŠ¤íƒ€ì¼
                header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
                header_font = Font(color='FFFFFF', bold=True)
                
                for cell in worksheet[1]:
                    cell.fill = header_fill
                    cell.font = header_font
                    cell.alignment = Alignment(horizontal='center')
                
                # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì •
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except:
                            pass
                    
                    adjusted_width = min(max_length + 2, 50)
                    worksheet.column_dimensions[column_letter].width = adjusted_width
            
            self.job.result_file = filename
            self.log(f"ğŸ“Š ì—‘ì…€ íŒŒì¼ ìƒì„±: {filename}.xlsx")
            
        except Exception as e:
            self.log(f"ì—‘ì…€ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def log(self, message: str):
        """ë¡œê·¸ ì¶”ê°€"""
        timestamp = datetime.now().strftime('%H:%M:%S')
        self.job.logs.append(f"[{timestamp}] {message}")
        print(f"[{self.job.id}] {message}")


# ==================== API ì—”ë“œí¬ì¸íŠ¸ ====================

@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€"""
    jobs = list(jobs_store.values())
    jobs.sort(key=lambda x: x.created_at, reverse=True)
    
    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "jobs": jobs[:20],  # ìµœê·¼ 20ê°œë§Œ í‘œì‹œ
            "active_count": sum(1 for j in jobs if j.status == CrawlStatus.RUNNING),
            "completed_count": sum(1 for j in jobs if j.status == CrawlStatus.COMPLETED),
            "total_collected": sum(len(j.data) for j in jobs)
        }
    )


@app.post("/jobs/create", response_class=HTMLResponse)
async def create_job(
    background_tasks: BackgroundTasks,
    name: str = Form(...),
    url: str = Form(...),
    selectors: str = Form(...)
):
    """ìƒˆ í¬ë¡¤ë§ ì‘ì—… ìƒì„±"""
    
    # ì„ íƒì íŒŒì‹±
    selector_dict = {}
    for line in selectors.strip().split('\n'):
        if ':' in line:
            parts = line.split(':', 1)
            if len(parts) == 2:
                field = parts[0].strip()
                selector = parts[1].strip()
                if field and selector:
                    selector_dict[field] = selector
    
    # ê¸°ë³¸ ì„ íƒì ì¶”ê°€ (ë¹„ì–´ìˆìœ¼ë©´)
    if not selector_dict:
        selector_dict = {
            'title': 'h1, h2, title',
            'content': 'p, article, .content',
            'links': 'a[href]'
        }
    
    # Job ìƒì„±
    job = CrawlJob(name=name, url=url, selectors=selector_dict)
    jobs_store[job.id] = job
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹œì‘
    crawler = SimpleCrawler(job)
    background_tasks.add_task(crawler.crawl)
    
    # HTMXìš© ì‘ì—… ì¹´ë“œ ë°˜í™˜
    return f"""
    <div id="job-{job.id}" 
         class="bg-white rounded-lg shadow p-6 border-l-4 border-blue-500 animate-pulse-border"
         hx-get="/jobs/{job.id}/status"
         hx-trigger="every 1s"
         hx-swap="outerHTML">
        <div class="flex justify-between items-start mb-4">
            <div class="flex-1 min-w-0">
                <h3 class="text-lg font-semibold truncate">{job.name}</h3>
                <p class="text-sm text-gray-500 truncate">{job.url}</p>
            </div>
            <span class="ml-2 px-3 py-1 bg-yellow-100 text-yellow-800 rounded-full text-sm">
                ì¤€ë¹„ ì¤‘
            </span>
        </div>
        <div class="w-full bg-gray-200 rounded-full h-2">
            <div class="bg-blue-500 h-2 rounded-full" style="width: 0%"></div>
        </div>
        <div class="mt-2 text-sm text-gray-600">
            í¬ë¡¤ë§ ì¤€ë¹„ ì¤‘...
        </div>
    </div>
    """


@app.get("/jobs/{job_id}/status", response_class=HTMLResponse)
async def get_job_status(job_id: str):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
    job = jobs_store.get(job_id)
    if not job:
        return "<div>ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤</div>"
    
    # ìƒíƒœë³„ ìƒ‰ìƒ
    colors = {
        CrawlStatus.PENDING: "yellow",
        CrawlStatus.RUNNING: "blue",
        CrawlStatus.COMPLETED: "green",
        CrawlStatus.FAILED: "red"
    }
    
    color = colors[job.status]
    
    # ì§„í–‰ ì¤‘ì´ ì•„ë‹ˆë©´ polling ì¤‘ì§€
    hx_attrs = 'hx-get="/jobs/' + job_id + '/status" hx-trigger="every 1s" hx-swap="outerHTML"' if job.status == CrawlStatus.RUNNING else ''
    
    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    download_button = ""
    if job.status == CrawlStatus.COMPLETED and job.result_file:
        download_button = f"""
        <div class="mt-4">
            <a href="/downloads/{job.result_file}.xlsx" 
               class="inline-flex items-center px-4 py-2 bg-green-500 text-white rounded hover:bg-green-600 transition-colors">
                <svg class="w-4 h-4 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                    <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M12 10v6m0 0l-3-3m3 3l3-3m2 8H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z"></path>
                </svg>
                ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ({len(job.data)}ê°œ í•­ëª©)
            </a>
        </div>
        """
    
    # ë¡œê·¸ í‘œì‹œ (ìµœê·¼ 5ê°œ)
    recent_logs = '<br>'.join(job.logs[-5:]) if job.logs else 'ë¡œê·¸ ì—†ìŒ'
    
    return f"""
    <div id="job-{job.id}" 
         class="bg-white rounded-lg shadow p-6 border-l-4 border-{color}-500"
         {hx_attrs}>
        <div class="flex justify-between items-start mb-4">
            <div class="flex-1 min-w-0">
                <h3 class="text-lg font-semibold truncate">{job.name}</h3>
                <p class="text-sm text-gray-500 truncate">{job.url}</p>
            </div>
            <span class="ml-2 px-3 py-1 bg-{color}-100 text-{color}-800 rounded-full text-sm">
                {job.status.value}
            </span>
        </div>
        
        <div class="w-full bg-gray-200 rounded-full h-2 mb-2">
            <div class="bg-{color}-500 h-2 rounded-full transition-all duration-300" 
                 style="width: {job.progress}%"></div>
        </div>
        
        <div class="flex justify-between text-sm text-gray-600">
            <span>ìˆ˜ì§‘: {job.collected_items}/{min(job.total_items, 11)}</span>
            <span>{job.progress}%</span>
        </div>
        
        {download_button}
        
        <details class="mt-4">
            <summary class="cursor-pointer text-sm text-blue-600 hover:text-blue-800">
                ë¡œê·¸ ë³´ê¸°
            </summary>
            <div class="mt-2 p-3 bg-gray-50 rounded text-xs font-mono text-gray-600">
                {recent_logs}
            </div>
        </details>
    </div>
    """


@app.post("/quick-crawl", response_class=HTMLResponse)
async def quick_crawl(
    background_tasks: BackgroundTasks,
    url: str = Form(...)
):
    """ë¹ ë¥¸ í¬ë¡¤ë§ (ìë™ ì„ íƒì)"""
    
    # ìë™ ì„ íƒì ì„¤ì •
    auto_selectors = {
        'title': 'h1, h2, h3, title',
        'description': 'meta[name="description"], p:first-of-type',
        'content': 'article, main, .content, .post-content, .entry-content',
        'images': 'img[src]',
        'links': 'a[href]'
    }
    
    # Job ìƒì„±
    domain = urlparse(url).netloc
    job = CrawlJob(
        name=f"Quick - {domain}",
        url=url,
        selectors=auto_selectors
    )
    jobs_store[job.id] = job
    
    # í¬ë¡¤ë§ ì‹œì‘
    crawler = SimpleCrawler(job)
    background_tasks.add_task(crawler.crawl)
    
    return f"""
    <div class="bg-green-50 border border-green-200 rounded-lg p-4">
        <div class="flex items-center">
            <svg class="w-5 h-5 text-green-600 mr-2" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"></path>
            </svg>
            <p class="text-green-800 font-semibold">ìë™ í¬ë¡¤ë§ ì‹œì‘!</p>
        </div>
        <p class="text-sm text-gray-600 mt-2">ì‘ì—… ID: {job.id}</p>
        <p class="text-sm text-gray-500">ìë™ ê°ì§€ëœ í•„ë“œ: {', '.join(auto_selectors.keys())}</p>
    </div>
    <script>
        setTimeout(() => location.reload(), 2000);
    </script>
    """


# ==================== ì‹¤í–‰ ====================
if __name__ == "__main__":
    import uvicorn
    
    print("\n" + "="*50)
    print("ğŸ•·ï¸  í¬ë¡¤ë§ ë§ˆìŠ¤í„° ì„œë²„ ì‹œì‘!")
    print("="*50)
    print("ğŸ“ ì£¼ì†Œ: http://localhost:8000")
    print("ğŸ“Š ì—‘ì…€ íŒŒì¼ì€ downloads í´ë”ì— ì €ì¥ë©ë‹ˆë‹¤")
    print("="*50 + "\n")
    
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)