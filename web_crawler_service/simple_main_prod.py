#!/usr/bin/env python3
"""
í”„ë¡œë•ì…˜ìš© FastAPI + HTMX í¬ë¡¤ë§ ì„œë¹„ìŠ¤
Fly.io ë°°í¬ì— ìµœì í™”ëœ ë²„ì „
"""

import os
from fastapi import FastAPI, Request, Form, BackgroundTasks
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import asyncio
import aiohttp
from bs4 import BeautifulSoup
import pandas as pd
import json
import uuid
from datetime import datetime
from typing import Optional, Dict, List, Any
from enum import Enum
from pathlib import Path
from urllib.parse import urlparse, urljoin
import openpyxl
from openpyxl.styles import PatternFill, Font, Alignment
from io import BytesIO

# ==================== í”„ë¡œë•ì…˜ ì„¤ì • ====================
# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸°
PORT = int(os.getenv("PORT", "8080"))
ENVIRONMENT = os.getenv("ENVIRONMENT", "production")
MAX_CONCURRENT_JOBS = int(os.getenv("MAX_CONCURRENT_JOBS", "20"))
MAX_PAGES_PER_JOB = int(os.getenv("MAX_PAGES_PER_JOB", "50"))
RATE_LIMIT_DELAY = float(os.getenv("RATE_LIMIT_DELAY", "0.5"))

class CrawlStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


class CrawlJob:
    def __init__(self, name: str, url: str, selectors: Dict[str, str]):
        self.id = str(uuid.uuid4())[:8]
        self.name = name
        self.url = url
        self.selectors = selectors
        self.status = CrawlStatus.PENDING
        self.progress = 0
        self.total_items = 0
        self.collected_items = 0
        self.created_at = datetime.now()
        self.result_file = None
        self.data = []
        self.logs = []


# ==================== ì•± ì´ˆê¸°í™” ====================
app = FastAPI(
    title="Crawling Master Service",
    description="í”„ë¡œë•ì…˜ í¬ë¡¤ë§ ì„œë¹„ìŠ¤",
    version="1.0.0"
)

templates = Jinja2Templates(directory="templates")

# ì „ì—­ ì €ì¥ì†Œ (ë©”ëª¨ë¦¬) - í”„ë¡œë•ì…˜ì—ì„œëŠ” Redis ê¶Œì¥
jobs_store: Dict[str, CrawlJob] = {}

# ë””ë ‰í† ë¦¬ ìƒì„±
Path("templates").mkdir(exist_ok=True)
Path("downloads").mkdir(exist_ok=True)
app.mount("/downloads", StaticFiles(directory="downloads"), name="downloads")


# ==================== í¬ë¡¤ëŸ¬ ì—”ì§„ ====================
class SimpleCrawler:
    """í”„ë¡œë•ì…˜ìš© ë¹„ë™ê¸° í¬ë¡¤ëŸ¬"""
    
    def __init__(self, job: CrawlJob):
        self.job = job
        self.session = None
        
    async def crawl(self):
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        try:
            self.job.status = CrawlStatus.RUNNING
            self.log(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: {self.job.url}")
            
            # íƒ€ì„ì•„ì›ƒê³¼ í—¤ë” ì„¤ì •
            timeout = aiohttp.ClientTimeout(total=30)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            async with aiohttp.ClientSession(timeout=timeout, headers=headers) as self.session:
                # ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
                html = await self.fetch_page(self.job.url)
                if not html:
                    raise Exception("í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                
                # ë°ì´í„° ì¶”ì¶œ
                soup = BeautifulSoup(html, 'html.parser')
                
                # ì„ íƒìë³„ë¡œ ë°ì´í„° ì¶”ì¶œ
                main_data = {}
                for field, selector in self.job.selectors.items():
                    try:
                        elements = soup.select(selector)
                        if elements:
                            if len(elements) > 1:
                                main_data[field] = [el.get_text(strip=True) for el in elements[:10]]
                            else:
                                main_data[field] = elements[0].get_text(strip=True)
                        else:
                            main_data[field] = ""
                    except Exception as e:
                        main_data[field] = f"Error: {str(e)}"
                        self.log(f"âš ï¸ {field} ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
                
                main_data['url'] = self.job.url
                main_data['crawled_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                self.job.data.append(main_data)
                
                # ì¶”ê°€ í˜ì´ì§€ ì°¾ê¸° (í”„ë¡œë•ì…˜: ì œí•œëœ ìˆ˜ë§Œ)
                links = self.find_similar_links(soup, self.job.url)
                max_pages = min(len(links), MAX_PAGES_PER_JOB)
                self.job.total_items = max_pages + 1
                
                # ì œí•œëœ ìˆ˜ì˜ ì¶”ê°€ í˜ì´ì§€ë§Œ í¬ë¡¤ë§
                for idx, link in enumerate(links[:max_pages], 1):
                    self.job.progress = int((idx / (max_pages + 1)) * 100)
                    self.log(f"ğŸ“„ í˜ì´ì§€ {idx}/{max_pages} í¬ë¡¤ë§ ì¤‘...")
                    
                    page_html = await self.fetch_page(link)
                    if page_html:
                        page_soup = BeautifulSoup(page_html, 'html.parser')
                        page_data = {}
                        
                        for field, selector in self.job.selectors.items():
                            try:
                                elements = page_soup.select(selector)
                                if elements:
                                    page_data[field] = elements[0].get_text(strip=True)
                                else:
                                    page_data[field] = ""
                            except:
                                page_data[field] = ""
                        
                        page_data['url'] = link
                        page_data['crawled_at'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        self.job.data.append(page_data)
                        self.job.collected_items += 1
                    
                    await asyncio.sleep(RATE_LIMIT_DELAY)  # Rate limiting
                
                # ì—‘ì…€ íŒŒì¼ ìƒì„±
                if self.job.data:
                    self.save_to_excel()
                
                self.job.status = CrawlStatus.COMPLETED
                self.job.progress = 100
                self.log(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ! {len(self.job.data)}ê°œ í•­ëª© ìˆ˜ì§‘")
                
        except Exception as e:
            self.job.status = CrawlStatus.FAILED
            self.log(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}")
    
    async def fetch_page(self, url: str) -> Optional[str]:
        """í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            async with self.session.get(url) as response:
                if response.status == 200:
                    return await response.text()
                else:
                    self.log(f"HTTP {response.status}: {url}")
                    return None
        except Exception as e:
            self.log(f"í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {url} - {str(e)}")
            return None
    
    def find_similar_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """ìœ ì‚¬í•œ ë§í¬ ì°¾ê¸°"""
        links = []
        parsed_base = urlparse(base_url)
        
        for a in soup.find_all('a', href=True):
            href = a['href']
            # ì ˆëŒ€ URLë¡œ ë³€í™˜
            full_url = urljoin(base_url, href)
            parsed = urlparse(full_url)
            
            # ê°™ì€ ë„ë©”ì¸ì˜ ë§í¬ë§Œ
            if parsed.netloc == parsed_base.netloc:
                if full_url not in links and full_url != base_url:
                    links.append(full_url)
        
        return links[:MAX_PAGES_PER_JOB]  # í”„ë¡œë•ì…˜: ì œí•œ
    
    def save_to_excel(self):
        """ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥"""
        if not self.job.data:
            return
        
        df = pd.DataFrame(self.job.data)
        
        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{self.job.name.replace(' ', '_')}_{timestamp}.xlsx"
        filepath = Path("downloads") / filename
        
        # ìŠ¤íƒ€ì¼ ì ìš©í•˜ì—¬ ì €ì¥
        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='í¬ë¡¤ë§ ê²°ê³¼')
            
            # ìŠ¤íƒ€ì¼ ì ìš©
            workbook = writer.book
            worksheet = writer.sheets['í¬ë¡¤ë§ ê²°ê³¼']
            
            # í—¤ë” ìŠ¤íƒ€ì¼
            header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
            header_font = Font(color='FFFFFF', bold=True)
            
            for cell in worksheet[1]:
                cell.fill = header_fill
                cell.font = header_font
                cell.alignment = Alignment(horizontal='center')
            
            # ì—´ ë„ˆë¹„ ìë™ ì¡°ì •
            for column in worksheet.columns:
                max_length = 0
                column = [cell for cell in column]
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column[0].column_letter].width = adjusted_width
        
        self.job.result_file = filename
        self.log(f"ğŸ’¾ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filename}")
    
    def log(self, message: str):
        """ë¡œê·¸ ê¸°ë¡"""
        log_entry = f"[{datetime.now().strftime('%H:%M:%S')}] {message}"
        self.job.logs.append(log_entry)
        if ENVIRONMENT != "production":
            print(log_entry)


# ==================== í—¬í¼ í•¨ìˆ˜ ====================
def auto_detect_selectors(html: str) -> Dict[str, str]:
    """HTMLì—ì„œ ìë™ìœ¼ë¡œ ì„ íƒì ê°ì§€"""
    soup = BeautifulSoup(html, 'html.parser')
    selectors = {}
    
    # ì œëª© íŒ¨í„´
    title_patterns = ['h1', 'h2', '.title', '.headline', '[class*="title"]', '[class*="heading"]']
    for pattern in title_patterns:
        if soup.select(pattern):
            selectors['title'] = pattern
            break
    
    # ë‚´ìš© íŒ¨í„´
    content_patterns = ['article', '.content', '.body', 'main', '.post-content', '[class*="content"]']
    for pattern in content_patterns:
        if soup.select(pattern):
            selectors['content'] = pattern
            break
    
    # ë‚ ì§œ íŒ¨í„´
    date_patterns = ['time', '.date', '.timestamp', '[datetime]', '[class*="date"]']
    for pattern in date_patterns:
        if soup.select(pattern):
            selectors['date'] = pattern
            break
    
    # ì´ë¯¸ì§€
    if soup.select('img'):
        selectors['images'] = 'img'
    
    # ë§í¬
    if soup.select('a[href]'):
        selectors['links'] = 'a[href]'
    
    return selectors


# ==================== API ë¼ìš°íŠ¸ ====================
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€"""
    # í™œì„± ì‘ì—… ìˆ˜ ê³„ì‚°
    active_jobs = sum(1 for job in jobs_store.values() if job.status == CrawlStatus.RUNNING)
    
    return templates.TemplateResponse("index.html", {
        "request": request,
        "jobs": list(jobs_store.values()),
        "active_jobs": active_jobs,
        "max_jobs": MAX_CONCURRENT_JOBS,
        "environment": ENVIRONMENT
    })


@app.post("/jobs/create", response_class=HTMLResponse)
async def create_job(
    request: Request,
    background_tasks: BackgroundTasks,
    name: str = Form(...),
    url: str = Form(...),
    selectors: str = Form(...)
):
    """ìƒˆ í¬ë¡¤ë§ ì‘ì—… ìƒì„±"""
    
    # ë™ì‹œ ì‹¤í–‰ ì œí•œ ì²´í¬
    running_jobs = sum(1 for job in jobs_store.values() if job.status == CrawlStatus.RUNNING)
    if running_jobs >= MAX_CONCURRENT_JOBS:
        return HTMLResponse(
            content=f'<div class="text-red-500">âš ï¸ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—… ìˆ˜({MAX_CONCURRENT_JOBS})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.</div>'
        )
    
    # ì„ íƒì íŒŒì‹±
    selector_dict = {}
    for line in selectors.strip().split('\n'):
        if ':' in line:
            key, value = line.split(':', 1)
            selector_dict[key.strip()] = value.strip()
    
    # ì‘ì—… ìƒì„±
    job = CrawlJob(name=name, url=url, selectors=selector_dict)
    jobs_store[job.id] = job
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ í¬ë¡¤ë§ ì‹¤í–‰
    crawler = SimpleCrawler(job)
    background_tasks.add_task(crawler.crawl)
    
    # ì‘ì—… ì¹´ë“œ HTML ë°˜í™˜ (HTMXìš©)
    return templates.TemplateResponse("partials/job_card.html", {
        "request": request,
        "job": job
    })


@app.get("/jobs/{job_id}/status", response_class=HTMLResponse)
async def get_job_status(request: Request, job_id: str):
    """ì‘ì—… ìƒíƒœ ì¡°íšŒ (HTMX pollingìš©)"""
    job = jobs_store.get(job_id)
    if not job:
        return HTMLResponse(content="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    return templates.TemplateResponse("partials/job_status.html", {
        "request": request,
        "job": job
    })


@app.post("/quick-crawl", response_class=HTMLResponse)
async def quick_crawl(
    request: Request,
    background_tasks: BackgroundTasks,
    url: str = Form(...)
):
    """ë¹ ë¥¸ ìë™ í¬ë¡¤ë§"""
    
    # URLë¡œ ê°„ë‹¨í•œ HTML ê°€ì ¸ì˜¤ê¸°
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                html = await response.text()
        
        # ìë™ ì„ íƒì ê°ì§€
        selectors = auto_detect_selectors(html)
        
        if not selectors:
            return HTMLResponse(
                content='<div class="text-yellow-500">âš ï¸ ìë™ ê°ì§€ëœ ì„ íƒìê°€ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.</div>'
            )
        
        # ì‘ì—… ìƒì„±
        job = CrawlJob(name=f"ìë™_{urlparse(url).netloc}", url=url, selectors=selectors)
        jobs_store[job.id] = job
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        crawler = SimpleCrawler(job)
        background_tasks.add_task(crawler.crawl)
        
        return templates.TemplateResponse("partials/job_card.html", {
            "request": request,
            "job": job
        })
        
    except Exception as e:
        return HTMLResponse(
            content=f'<div class="text-red-500">âŒ ì˜¤ë¥˜: {str(e)}</div>'
        )


@app.get("/downloads/{filename}")
async def download_file(filename: str):
    """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
    file_path = Path("downloads") / filename
    if file_path.exists():
        return FileResponse(
            path=file_path,
            media_type='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
            filename=filename
        )
    return HTMLResponse(content="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", status_code=404)


@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "status": "healthy",
        "environment": ENVIRONMENT,
        "active_jobs": sum(1 for job in jobs_store.values() if job.status == CrawlStatus.RUNNING),
        "total_jobs": len(jobs_store)
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=PORT)